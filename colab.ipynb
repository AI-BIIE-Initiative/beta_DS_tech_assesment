{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "6z-YDR59ZxU-"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Load trivia dataset\n",
        "dataset = load_dataset(\"vapit/HarryPotterQA\")['train']\n",
        "validation_dataset = load_dataset(\"vapit/HarryPotterQA\")['validation'].sample(100)\n",
        "\n",
        "print(f\" Train split: {len(dataset)} questions\")\n",
        "print(f\" Validation split: {len(validation_dataset)} questions\")\n",
        "\n",
        "# Load GPT Distil\n",
        "model_name = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 848,
          "referenced_widgets": [
            "2ab1dab12f584b5693d05b8781a1d4e6",
            "e8fa625d14ea4e929856fd6def372e6e",
            "5466cebd94d44d4f9b039ccfb0e9f36d",
            "ef0a8acdcdf64286838a4fdb7f5d0510",
            "6ec0261c6c214660b3c4818b273a2516",
            "ad7d7be1461841c8af4fc5b7d7dab773",
            "2ca64ccb15434da69f2aa94c0fcc096f",
            "079c763bd67b405a9cd203d9fec9fbfa",
            "7d16ffe8db5e4ea784736366791c9e2f",
            "f40ed10a208743a993f67910d6c0dacb",
            "1f0d4716c5e14db1bfb87b249eecad4e",
            "3f6d2b01a72f4aa09ab90370e202041d",
            "1f71e742abeb41a98ffef91e31a43508",
            "969ee8d23408406aa1bdcdeef8a90823",
            "df280ff919c84adfb4f95ae6d7111eca",
            "8f44fd49800544c89d73ea4ebe2c1246",
            "e5fceee6efce43f8a62588f383b7143c",
            "e53be6d999624e93acbf5e6448ff26a5",
            "b6aa63d6e98e49f48fbbcef7a3397206",
            "5194694c1d6a45f384987d4f6c47d06f",
            "123196b441fe4d138e43558a4f42dea7",
            "4672a59ed3e04d1b8b9d9e6f98353c63",
            "e09059fc8bfa46358223263199c03a49",
            "c6c250958a5e4ef7ac8706377c9940bd",
            "ec79967fff9a43868fcf21aebba88444",
            "aac42f094c0f4baa9f011f8be29c55c8",
            "63cdf3ac081043618dcffa9486b398a1",
            "5cd3521cc9d14b7ab57a4c801ba8d5f9",
            "fccb433b106c466db0793daf68e8940d",
            "ad651ac8ae4a4e3080fc16b3816208b8",
            "d311b49b946c485f81c189d26e4a9d8a",
            "62ecd46d06b94eb791e29b404a8f0e9d",
            "c604bdb4f23a46749545a2a911296f14",
            "fd62427eb68b4b66ba130db0c7b0add5",
            "f615e3a975ca43a9a9ddfe98128c9345",
            "78b201eebc3644a5b7a6b4e8c4411798",
            "5cebf8fce96d4b8bb7e425ea7826c613",
            "e112a1f874664a27b1867bc8c9c8f120",
            "9f08e7c719a7481aa7cec8e468ffb883",
            "12c4b617eca04c6cb605411d2013de3b",
            "124b971d3c2742e3b9e90902e4fb6c9c",
            "d2fe72726edc4687880198de3d20d341",
            "de6fc6473b564369a55ebbe9bd4dbfcb",
            "4fa4dae32ceb42bba3f538b5c915b1be",
            "734417dd9b9f40f6b32b9d070b33f278",
            "3b634886823b4255981b34e63aa66c85",
            "220156c0862d4d46b89a19f5020bb402",
            "3d15c29d4fdf49f29813a8cde8383b30",
            "312b9de34e004dd181b907014ab4a39f",
            "6069d171b17e4b90931d977f43ad4456",
            "17aaec6193a2431895220c627e131b3e",
            "9851c0a32e7f44568419abed5aea68ae",
            "a6e7222521fd4e2da7b1c7e29e891e90",
            "8293e1731a4d4f019b0e14e741c1459c",
            "af0b3e7089dc4b8d851b6317f8292684"
          ]
        },
        "id": "tMK6OqeQyX0S",
        "outputId": "ac9e58fe-06e2-44c7-a7ce-07a5db479ed0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/2.34k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ab1dab12f584b5693d05b8781a1d4e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f6d2b01a72f4aa09ab90370e202041d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Downloading data:   0%|          | 0.00/7.12M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.12M/7.12M [00:00<00:00, 15.1MB/s]\n",
            "\n",
            "Downloading data:   0%|          | 0.00/797k [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 797k/797k [00:00<00:00, 3.58MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e09059fc8bfa46358223263199c03a49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/25999 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd62427eb68b4b66ba130db0c7b0add5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/2889 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "734417dd9b9f40f6b32b9d070b33f278"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Loading a dataset cached in a LocalFileSystem is not supported.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3b912db1618d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load trivia dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vapit/HarryPotterQA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mvalidation_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vapit/HarryPotterQA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         \u001b[0mkeep_in_memory\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeep_in_memory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mis_small_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilder_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m     )\n\u001b[0;32m-> 2149\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverification_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverification_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2150\u001b[0m     \u001b[0;31m# Rename and cast features to match task schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36mas_dataset\u001b[0;34m(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[0mis_local\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_remote_filesystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_local\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading a dataset cached in a {type(self._fs).__name__} is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             raise FileNotFoundError(\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Loading a dataset cached in a LocalFileSystem is not supported."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(text):\n",
        "    return re.sub(r\"[^a-z0-9]+\", \" \", text.lower()).strip()\n",
        "\n",
        "def compute_f1(pred, truth):\n",
        "    pred_tokens = normalize(pred).split()\n",
        "    truth_tokens = normalize(truth).split()\n",
        "    common = set(pred_tokens) & set(truth_tokens)\n",
        "    if not common:\n",
        "        return 0.0\n",
        "    precision = len(common) / len(pred_tokens)\n",
        "    recall = len(common) / len(truth_tokens)\n",
        "    return 2 * precision * recall / (precision + recall)"
      ],
      "metadata": {
        "id": "UyGOpXTnHICK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(dataset, n_samples=50):\n",
        "    exact_match = 0\n",
        "    f1_scores = []\n",
        "\n",
        "    for i in tqdm(range(n_samples)):\n",
        "        item = dataset[i]\n",
        "        question = item[\"question\"]\n",
        "        gold = item[\"answer\"]\n",
        "\n",
        "        prompt = f\"Answer this Harry Potter trivia question:\\n{question}\\nAnswer:\"\n",
        "        output = generator(prompt, max_length=len(tokenizer.encode(prompt)) + 20, do_sample=False)[0][\"generated_text\"]\n",
        "        generated = output.split(\"Answer:\")[-1].strip().split(\"\\n\")[0]\n",
        "\n",
        "        norm_pred = normalize(generated)\n",
        "        norm_gold = normalize(gold)\n",
        "\n",
        "        print(f\"\\nðŸ“˜ Q: {question}\")\n",
        "        print(f\"âœ… GT: {gold}\")\n",
        "        print(f\"ðŸ¤– Pred: {generated}\")\n",
        "\n",
        "        if norm_pred == norm_gold:\n",
        "            exact_match += 1\n",
        "        f1_scores.append(compute_f1(norm_pred, norm_gold))\n",
        "\n",
        "    em = exact_match / n_samples\n",
        "    f1 = np.mean(f1_scores)\n",
        "    print(f\"\\nðŸŽ¯ Exact Match: {em:.2%}\")\n",
        "    print(f\"ðŸŽ¯ Average F1: {f1:.2%}\")\n"
      ],
      "metadata": {
        "id": "xpytRWVUHqf5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(validation_dataset, n_samples=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIQNZy07HuFa",
        "outputId": "a0687562-69d4-4323-f8c7-572d80acd2bb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=45) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "  2%|â–         | 1/50 [00:01<01:26,  1.76s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=42) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: How do the Quidditch team members feel about their new lineup?\n",
            "âœ… GT: They are pleased to be rid of McLaggen and glad to have Katie back, resulting in excellent Quidditch practices under Harry's leadership.\n",
            "ðŸ¤– Pred: I think they're all very happy with the new lineup. I think they're all very happy with the new lineup. I think they're all very happy with the new lineup. I think they're all very happy with the new lineup. I think they're all very happy with the new lineup. I think they're all very happy with the new lineup. I think they're all very happy with the new lineup. I think they're all very happy with the new lineup. I think they're all very happy with the new lineup. I think they're all very happy with the new lineup. I think they're all very happy with the new lineup. I think they're all very happy with the new lineup. I think they're all very happy with the new lineup. I think they're all very happy with the new lineup. I think they're all very happy with the new lineup. I think they're all very happy with the new lineup. I think they're all very happy with the new lineup. I think they're all very happy with the new lineup. I think they're all very happy with the new lineup. I think they're all very happy with the new lineup. I think they're all very happy with the new lineup. I think they\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|â–         | 2/50 [00:03<01:15,  1.56s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: How does Sirius Black advise Ron to evaluate Crouch?\n",
            "âœ… GT: Sirius advises Ron to observe Crouch's behavior towards his inferiors to understand his true nature.\n",
            "ðŸ¤– Pred: Ron is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a very good friend of Ron. He is a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|â–Œ         | 3/50 [00:04<01:10,  1.50s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: I think it's because the author is trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|â–Š         | 4/50 [00:06<01:07,  1.47s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: I think it's because the author is trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|â–ˆ         | 5/50 [00:07<01:05,  1.46s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What potential significance could Harry's questions hold?\n",
            "âœ… GT: Harry's questions about Malfoy and Snape may hint at important connections or secrets that will be revealed later in the story.\n",
            "ðŸ¤– Pred: Harry Potter is a very interesting question. It is a question that has been asked many times in the past. It is a question that has been asked many times in the past. It is a question that has been asked many times in the past. It is a question that has been asked many times in the past. It is a question that has been asked many times in the past. It is a question that has been asked many times in the past. It is a question that has been asked many times in the past. It is a question that has been asked many times in the past. It is a question that has been asked many times in the past. It is a question that has been asked many times in the past. It is a question that has been asked many times in the past. It is a question that has been asked many times in the past. It is a question that has been asked many times in the past. It is a question that has been asked many times in the past. It is a question that has been asked many times in the past. It is a question that has been asked many times in the past. It is a question that has been asked many times in the past. It is a question that has been asked many\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|â–ˆâ–        | 6/50 [00:08<01:03,  1.45s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=41) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: I think it's because the author is trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|â–ˆâ–        | 7/50 [00:10<01:03,  1.49s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=41) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: How can individuals protect themselves according to the passage?\n",
            "âœ… GT: By observing simple security guidelines, individuals can protect themselves, their families, and their homes from attack by the Death Eaters.\n",
            "ðŸ¤– Pred: The answer is that the person who is protecting themselves is the person who is protecting himself.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|â–ˆâ–Œ        | 8/50 [00:12<01:07,  1.61s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What spell did Fred use against the Death Eater?\n",
            "âœ… GT: Fred used three Stunning Spells against the Death Eater.\n",
            "ðŸ¤– Pred: Harry Potter:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|â–ˆâ–Š        | 9/50 [00:13<01:03,  1.56s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: I think it's because the author is trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|â–ˆâ–ˆ        | 10/50 [00:15<01:00,  1.52s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: I think it's because the author is trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|â–ˆâ–ˆâ–       | 11/50 [00:16<00:58,  1.50s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=45) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: I think it's because the author is trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|â–ˆâ–ˆâ–       | 12/50 [00:18<00:56,  1.48s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Where do Harry, Ron, and Hermione sit in the common room?\n",
            "âœ… GT: They sit apart from everyone else by a dark window that is gradually filling up with snow.\n",
            "ðŸ¤– Pred: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|â–ˆâ–ˆâ–Œ       | 13/50 [00:19<00:54,  1.47s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: I think it's because the author is trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|â–ˆâ–ˆâ–Š       | 14/50 [00:20<00:52,  1.46s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=43) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: I think it's because the author is trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [00:22<00:51,  1.48s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=39) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is a key characteristic of the people in the passage?\n",
            "âœ… GT: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them.\n",
            "ðŸ¤– Pred: The key characteristic of the people in the passage is that they\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [00:24<00:54,  1.60s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=42) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: How does Voldemort appear differently this time?\n",
            "âœ… GT: Unlike before, Voldemort's features are burned and blurred, waxy and oddly distorted, with a permanently bloody look in the eyes, even though the pupils are not yet slits.\n",
            "ðŸ¤– Pred: Harry Potter is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard. He is a wizard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [00:25<00:50,  1.54s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Who is playing poorly at the Quidditch match?\n",
            "âœ… GT: The person currently defending the goalposts is performing poorly.\n",
            "ðŸ¤– Pred: Harry Potter is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is a very good player. He is\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [00:27<00:48,  1.50s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=44) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: How does Snape express his dislike for Harry?\n",
            "âœ… GT: Snape enjoys taking points away from Harry and suggesting his suspension from Hogwarts.\n",
            "ðŸ¤– Pred: Harry Potter is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [00:28<00:45,  1.48s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=44) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is the speaker planning to do with the Horcrux?\n",
            "âœ… GT: They plan to destroy the Horcrux as soon as they can.\n",
            "ðŸ¤– Pred: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [00:30<00:43,  1.46s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=43) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What effect does Harry's action have on the marble headstone?\n",
            "âœ… GT: The marble headstone cracks as Harry hides behind it during the attack.\n",
            "ðŸ¤– Pred: Harry Potter is a very powerful and powerful magical power. It is a powerful magical power that is very powerful and powerful. It is a powerful magical power that is very powerful and powerful. It is a powerful magical power that is very powerful and powerful. It is a powerful magical power that is very powerful and powerful. It is a powerful magical power that is very powerful and powerful. It is a powerful magical power that is very powerful and powerful. It is a powerful magical power that is very powerful and powerful. It is a powerful magical power that is very powerful and powerful. It is a powerful magical power that is very powerful and powerful. It is a powerful magical power that is very powerful and powerful. It is a powerful magical power that is very powerful and powerful. It is a powerful magical power that is very powerful and powerful. It is a powerful magical power that is very powerful and powerful. It is a powerful magical power that is very powerful and powerful. It is a powerful magical power that is very powerful and powerful. It is a powerful magical power that is very powerful and powerful. It is a powerful magical power that is very powerful and powerful. It is a powerful magical power that is very powerful and powerful. It is a powerful magical power that is very powerful\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [00:31<00:42,  1.45s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is a key characteristic of the people in the passage?\n",
            "âœ… GT: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them.\n",
            "ðŸ¤– Pred: The key characteristic of the people in the passage is that they\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [00:32<00:40,  1.44s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=41) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: I think it's because the author is trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [00:34<00:39,  1.46s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=45) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is Dean doing when he enters the room?\n",
            "âœ… GT: Dean enters the room and swears loudly upon seeing something.\n",
            "ðŸ¤– Pred: Dean is a student of the Hogwarts School of Witchcraft and Wizardry. He is a member of the Hogwarts School of Witchcraft and Wizardry. He is a member of the Hogwarts School of Witchcraft and Wizardry. He is a member of the Hogwarts School of Witchcraft and Wizardry. He is a member of the Hogwarts School of Witchcraft and Wizardry. He is a member of the Hogwarts School of Witchcraft and Wizardry. He is a member of the Hogwarts School of Witchcraft and Wizardry. He is a member of the Hogwarts School of Witchcraft and Wizardry. He is a member of the Hogwarts School of Witchcraft and Wizardry. He is a member of the Hogwarts School of Witchcraft and Wizardry. He is a member of the Hogwarts School of Witchcraft and Wizardry. He is a member of the Hogwarts School of Witchcraft and Wizardry. He is a member of the Hogwarts School of Witchcraft and Wizardry. He is a member of the Hogwarts School of Witchcraft and Wizardry. He is a member of the Hogwarts School of Witchcraft and Wizardry. He is a member of the Hogwarts School of Witchcraft and Wizardry. He is a member of the Hogwarts School of Witchcraft and Wizardry.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [00:36<00:41,  1.60s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=41) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What impact does the weather have on the overall atmosphere of the scene?\n",
            "âœ… GT: The poor visibility and dimming light create an ominous and foreboding atmosphere, heightening the tension and suspense leading up to the Quidditch match.\n",
            "ðŸ¤– Pred: The weather is a major factor in the overall atmosphere of the scene. The weather is a major factor in the overall atmosphere of the scene. The weather is a major factor in the overall atmosphere of the scene. The weather is a major factor in the overall atmosphere of the scene. The weather is a major factor in the overall atmosphere of the scene. The weather is a major factor in the overall atmosphere of the scene. The weather is a major factor in the overall atmosphere of the scene. The weather is a major factor in the overall atmosphere of the scene. The weather is a major factor in the overall atmosphere of the scene. The weather is a major factor in the overall atmosphere of the scene. The weather is a major factor in the overall atmosphere of the scene. The weather is a major factor in the overall atmosphere of the scene. The weather is a major factor in the overall atmosphere of the scene. The weather is a major factor in the overall atmosphere of the scene. The weather is a major factor in the overall atmosphere of the scene. The weather is a major factor in the overall atmosphere of the scene. The weather is a major factor in the overall atmosphere of the scene. The weather is a major factor in the overall atmosphere of the scene. The weather is\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [00:37<00:38,  1.55s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=41) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: How does Lupin react when Harry mentions Black?\n",
            "âœ… GT: Lupin seems reluctant to discuss Black, as evidenced by Harry's observation that 'Lupin was obviously not keen on the subject.'\n",
            "ðŸ¤– Pred: \"I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [00:39<00:36,  1.52s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=42) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What effect does Hermione's behavior have on Harry?\n",
            "âœ… GT: Harry finds Hermione's behavior during questioning to be off-putting, making it difficult for him to focus on the conversation.\n",
            "ðŸ¤– Pred: Hermione's behavior is a result of her being a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family. She is a member of the family\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [00:40<00:34,  1.49s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=44) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is the reward for the student who performs best?\n",
            "âœ… GT: The winning student will receive a small hedgehog named Felix as a prize.\n",
            "ðŸ¤– Pred: The reward for the student who performs best is the reward for the student who performs best.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [00:42<00:32,  1.47s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=46) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What motivates Harry to sit down and listen to the others?\n",
            "âœ… GT: Although Harry is still angry, his need for information drives him to listen to what the others have to say.\n",
            "ðŸ¤– Pred: I think it's a good idea to\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [00:43<00:30,  1.46s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What kinds of negative effects can come from reading certain books according to Ron?\n",
            "âœ… GT: Ron mentions that some books his father's friends have read had negative consequences such as burning out eyes or making readers speak in limericks for the rest of their lives.\n",
            "ðŸ¤– Pred: The answer is:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [00:44<00:29,  1.46s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=52) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: How does Harry react to the potential danger?\n",
            "âœ… GT: Harry is concerned and believes that it's worth investigating, despite Ron's objections.\n",
            "ðŸ¤– Pred: Harry is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a very good person. He is a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [00:46<00:28,  1.49s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=41) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What was Mr. and Mrs. Dursley's reason for not wanting owls to follow them?\n",
            "âœ… GT: They were on a secret mission and did not want unwanted attention. Dementors also accompanied the owls, adding to their concern.\n",
            "ðŸ¤– Pred: Mr.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [00:48<00:29,  1.62s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=42) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why can't Hermione move despite Harry's urging?\n",
            "âœ… GT: Hermione is frozen with fear and unable to move, despite Harry's attempts to pull her towards safety.\n",
            "ðŸ¤– Pred: Harry Potter is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is a very good student. He is\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [00:49<00:26,  1.56s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=43) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is Dumbledore's responsibility towards Harry regarding Hagrid?\n",
            "âœ… GT: Dumbledore has a duty to warn Harry and his fellow students about the potential dangers of associating with Hagrid due to his friendship with the boy who brought down Voldemort.\n",
            "ðŸ¤– Pred: Harry Potter is responsible for the Harry Potter and the Deathly Hallows.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [00:51<00:24,  1.52s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is a key characteristic of the people in the passage?\n",
            "âœ… GT: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them.\n",
            "ðŸ¤– Pred: The key characteristic of the people in the passage is that they\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [00:52<00:22,  1.49s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=43) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: I think it's because the author is trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [00:54<00:20,  1.47s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=43) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Who is sitting next to Hermione when she enters the room?\n",
            "âœ… GT: Professor Umbridge is sitting next to Hermione when she enters the room.\n",
            "ðŸ¤– Pred: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [00:55<00:18,  1.46s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is a key characteristic of the people in the passage?\n",
            "âœ… GT: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them.\n",
            "ðŸ¤– Pred: The key characteristic of the people in the passage is that they\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [00:56<00:17,  1.44s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=43) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: How does the shield affect Harry and Ron?\n",
            "âœ… GT: The shield forces Harry and Ron backwards a few steps and makes them glare at each other through it, indicating a deep rift between them.\n",
            "ðŸ¤– Pred: Harry and Ron are both very good friends.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [00:58<00:16,  1.46s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=39) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is a key characteristic of the people in the passage?\n",
            "âœ… GT: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them.\n",
            "ðŸ¤– Pred: The key characteristic of the people in the passage is that they\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [01:00<00:16,  1.61s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is the weather like during breakfast?\n",
            "âœ… GT: A clear, pale blue sky is seen through the Great Hall ceiling, indicating good weather.\n",
            "ðŸ¤– Pred: The weather is very cold\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [01:01<00:13,  1.56s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=43) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: I think it's because the author is trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [01:03<00:12,  1.52s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=38) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is a key characteristic of the people in the passage?\n",
            "âœ… GT: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them.\n",
            "ðŸ¤– Pred: The key characteristic of the people in the passage is that they\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [01:04<00:10,  1.50s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=44) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What condition is the owl in?\n",
            "âœ… GT: The owl lies motionless and pathetic on the floor of its cage. Harry cannot understand what has happened to it.\n",
            "ðŸ¤– Pred: The owl is a small, small, and small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small, small,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [01:06<00:08,  1.47s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=41) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: According to Bill, what is the danger level of these people?\n",
            "âœ… GT: Bill believes that some of these people are really dangerous.\n",
            "ðŸ¤– Pred: The danger level of these people is that they are not going to be able to get to the point where they are going to be able to get to the point where they are going to be able to get to the point where they are going to be able to get to the point where they are going to be able to get to the point where they are going to be able to get to the point where they are going to be able to get to the point where they are going to be able to get to the point where they are going to be able to get to the point where they are going to be able to get to the point where they are going to be able to get to the point where they are going to be able to get to the point where they are going to be able to get to the point where they are going to be able to get to the point where they are going to be able to get to the point where they are going to be able to get to the point where they are going to be able to get to the point where they are going to be able to get to the point where they are going to be able to get to the point where they are going to be able to get to the point where they are going to be able\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [01:07<00:07,  1.46s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=43) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What did Professor Lupin promise to teach Harry?\n",
            "âœ… GT: Professor Lupin promised to teach Harry how to ward off Dementors during the coming week.\n",
            "ðŸ¤– Pred: Harry Potter is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is a very popular character in the Harry Potter series. He is\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [01:09<00:05,  1.46s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=43) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is a key characteristic of the people in the passage?\n",
            "âœ… GT: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them.\n",
            "ðŸ¤– Pred: The key characteristic of the people in the passage is that they\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [01:10<00:04,  1.47s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=45) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is a key characteristic of the people in the passage?\n",
            "âœ… GT: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them.\n",
            "ðŸ¤– Pred: The key characteristic of the people in the passage is that they\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [01:12<00:03,  1.61s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is Mrs. Weasley's motivation for confronting Mundungus Fletcher?\n",
            "âœ… GT: Mrs. Weasley wants to confront Mundungus Fletcher because he failed to follow Harry as instructed. She has also been eager to have a go at him since the incident.\n",
            "ðŸ¤– Pred: Mrs. Weasley is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a very good friend of Harry Potter. He is a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [01:13<00:01,  1.56s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: I think it's because the author is trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he's trying to make a point that he\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:15<00:00,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: How did Ginny Weasley react to her diary?\n",
            "âœ… GT: Ginny Weasley initially trusted her diary but later became suspicious and tried to dispose of it.\n",
            "ðŸ¤– Pred: I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle of a long night when I was in the middle\n",
            "\n",
            "ðŸŽ¯ Exact Match: 0.00%\n",
            "ðŸŽ¯ Average F1: 5.44%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_example(example):\n",
        "    if example[\"context\"].strip():\n",
        "        return {\n",
        "            \"text\": f\"Context: {example['context']}\\nQuestion: {example['question']}\\nAnswer: {example['answer']}\"\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            \"text\": f\"Question: {example['question']}\\nAnswer: {example['answer']}\"\n",
        "        }\n",
        "\n",
        "train_texts = dataset.map(format_example)\n",
        "val_texts = validation_dataset.map(format_example)"
      ],
      "metadata": {
        "id": "5F8ESX0yJbYi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # required for GPT-style models\n",
        "\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
        "\n",
        "train_tok = train_texts.map(tokenize, batched=True)\n",
        "val_tok = val_texts.map(tokenize, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9712dc81b14245e080525a92d46166ab",
            "535601458da045629532ab1b7863a3b0",
            "086ca99a87c54e038b05e9735c8eff58",
            "d333987962664b84a62185394aa73fa8",
            "0dfddb661bfc4847bc5ae80147942cf0",
            "a1bfa15fd300422d92e838b3c1513489",
            "2cab6d8f5398452ab45b49abfb688329",
            "1e7d658c04dd42bdbf8ed571d70ea6f0",
            "91193d6bcea74d07bc4f200a7395ba3a",
            "0689a7ccc54e422288e24fc82b294136",
            "3269598511e948f9ae623cbe1e4aa6e0"
          ]
        },
        "id": "brsJJSqjJ0XN",
        "outputId": "c00b53b3-2f21-40fa-db72-070936e7b371"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2889 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9712dc81b14245e080525a92d46166ab"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets accelerate peft"
      ],
      "metadata": {
        "id": "BD1kNIp9J61t"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(\"cuda\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"c_attn\"],  # GPT2-specific module\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TR0gCwMhKVXs",
        "outputId": "e0ef75c6-59ca-4e34-997f-902d9bce6c56"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 147,456 || all params: 82,060,032 || trainable%: 0.1797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./peft-gpt2-hp\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=5e-5,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=val_tok,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "C-5_HiqJKvch",
        "outputId": "00a93449-a413-4caf-bba3-7decd5586d47"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-28bc30420065>:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='19500' max='19500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [19500/19500 20:57, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>2.932000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>2.775300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>2.757000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=19500, training_loss=2.8214248798076924, metrics={'train_runtime': 1257.6863, 'train_samples_per_second': 62.016, 'train_steps_per_second': 15.505, 'total_flos': 5112756392951808.0, 'train_loss': 2.8214248798076924, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(text):\n",
        "    return re.sub(r\"\\W+\", \" \", text.lower()).strip()\n",
        "\n",
        "def compute_f1(pred, truth):\n",
        "    pred_tokens = normalize(pred).split()\n",
        "    truth_tokens = normalize(truth).split()\n",
        "    common = set(pred_tokens) & set(truth_tokens)\n",
        "    if not common:\n",
        "        return 0.0\n",
        "    precision = len(common) / len(pred_tokens)\n",
        "    recall = len(common) / len(truth_tokens)\n",
        "    return 2 * precision * recall / (precision + recall)"
      ],
      "metadata": {
        "id": "tZHUEBb0Ou4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "base = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(\"cuda\")\n",
        "model = PeftModel.from_pretrained(base, \"./peft-gpt2-hp/checkpoint-19500\")"
      ],
      "metadata": {
        "id": "7Wqb_EiTPWNQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(base, \"./peft-gpt2-hp/checkpoint-19500\")\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FegLUZW2P9Pv",
        "outputId": "a3d9bf1f-2d90-4d73-ab24-92549b4aac22"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(validation_dataset, n_samples=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyEj4QnhQIDH",
        "outputId": "952d97fc-b809-4418-f3cb-f4ced53dddd6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=45) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "  2%|â–         | 1/50 [00:02<02:04,  2.54s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=42) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: How do the Quidditch team members feel about their new lineup?\n",
            "âœ… GT: They are pleased to be rid of McLaggen and glad to have Katie back, resulting in excellent Quidditch practices under Harry's leadership.\n",
            "ðŸ¤– Pred: They feel that they have a better chance of winning the Quidditch World Cup than they did in the previous year. They feel that they have a better chance of winning the Quidditch World Cup than they did in the previous year. They feel that they have a better chance of winning the Quidditch World Cup than they did in the previous year. They feel that they have a better chance of winning the Quidditch World Cup than they did in the previous year. They feel that they have a better chance of winning the Quidditch World Cup than they did in the previous year. They feel that they have a better chance of winning the Quidditch World Cup than they did in the previous year. They feel that they have a better chance of winning the Quidditch World Cup than they did in the previous year. They feel that they have a better chance of winning the Quidditch World Cup than they did in the previous year. They feel that they have a better chance of winning the Quidditch World Cup than they did in the previous year. They feel that they have a better chance of winning the Quidditch World Cup than they did in the previous year. They feel that they have a better chance of winning the Quidditch World Cup\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|â–         | 2/50 [00:04<01:56,  2.43s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: How does Sirius Black advise Ron to evaluate Crouch?\n",
            "âœ… GT: Sirius advises Ron to observe Crouch's behavior towards his inferiors to understand his true nature.\n",
            "ðŸ¤– Pred: Sirius Black advises Ron to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him to evaluate Crouch by asking him\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|â–Œ         | 3/50 [00:06<01:40,  2.13s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|â–Š         | 4/50 [00:08<01:31,  1.99s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|â–ˆ         | 5/50 [00:10<01:29,  1.99s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What potential significance could Harry's questions hold?\n",
            "âœ… GT: Harry's questions about Malfoy and Snape may hint at important connections or secrets that will be revealed later in the story.\n",
            "ðŸ¤– Pred: Harry's question is a question about the nature of the universe and the nature of the universe. It is a question about the nature of the universe and the nature of the universe. It is a question about the nature of the universe and the nature of the universe. It is a question about the nature of the universe and the nature of the universe. It is a question about the nature of the universe and the nature of the universe. It is a question about the nature of the universe and the nature of the universe. It is a question about the nature of the universe and the nature of the universe. It is a question about the nature of the universe and the nature of the universe. It is a question about the nature of the universe and the nature of the universe. It is a question about the nature of the universe and the nature of the universe. It is a question about the nature of the universe and the nature of the universe. It is a question about the nature of the universe and the nature of the universe. It is a question about the nature of the universe and the nature of the universe. It is a question about the nature of the universe and the nature of the universe. It is a question about the nature of the universe and the nature of the universe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|â–ˆâ–        | 6/50 [00:12<01:28,  2.02s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=41) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|â–ˆâ–        | 7/50 [00:14<01:23,  1.93s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=41) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: How can individuals protect themselves according to the passage?\n",
            "âœ… GT: By observing simple security guidelines, individuals can protect themselves, their families, and their homes from attack by the Death Eaters.\n",
            "ðŸ¤– Pred: They protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the passage by saying that they protect themselves according to the\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|â–ˆâ–Œ        | 8/50 [00:16<01:18,  1.88s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What spell did Fred use against the Death Eater?\n",
            "âœ… GT: Fred used three Stunning Spells against the Death Eater.\n",
            "ðŸ¤– Pred: Fred used the Death Eater to defeat the Death Eater. Fred used the Death Eater to defeat the Death Eater. Fred used the Death Eater to defeat the Death Eater. Fred used the Death Eater to defeat the Death Eater. Fred used the Death Eater to defeat the Death Eater. Fred used the Death Eater to defeat the Death Eater. Fred used the Death Eater to defeat the Death Eater. Fred used the Death Eater to defeat the Death Eater. Fred used the Death Eater to defeat the Death Eater. Fred used the Death Eater to defeat the Death Eater. Fred used the Death Eater to defeat the Death Eater. Fred used the Death Eater to defeat the Death Eater. Fred used the Death Eater to defeat the Death Eater. Fred used the Death Eater to defeat the Death Eater. Fred used the Death Eater to defeat the Death Eater. Fred used the Death Eater to defeat the Death Eater. Fred used the Death Eater to defeat the Death Eater. Fred used the Death Eater to defeat the Death Eater. Fred used the Death Eater to defeat the Death Eater\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|â–ˆâ–Š        | 9/50 [00:17<01:16,  1.85s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|â–ˆâ–ˆ        | 10/50 [00:19<01:12,  1.82s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|â–ˆâ–ˆâ–       | 11/50 [00:21<01:10,  1.80s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=45) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|â–ˆâ–ˆâ–       | 12/50 [00:23<01:14,  1.95s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Where do Harry, Ron, and Hermione sit in the common room?\n",
            "âœ… GT: They sit apart from everyone else by a dark window that is gradually filling up with snow.\n",
            "ðŸ¤– Pred: Harry sits in the common room, and his parents sit in the common room. He is the only person in the room who is not in the common room. He is the only person in the room who is not in the common room. He is the only person in the room who is not in the common room. He is the only person in the room who is not in the common room. He is the only person in the room who is not in the common room. He is the only person in the room who is not in the common room. He is the only person in the room who is not in the common room. He is the only person in the room who is not in the common room. He is the only person in the room who is not in the common room. He is the only person in the room who is not in the common room. He is the only person in the room who is not in the common room. He is the only person in the room who is not in the common room. He is the only person in the room who is not in the common room. He is the only person in the room who is not in the common room. He is the only person in the room who is not in the common room.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|â–ˆâ–ˆâ–Œ       | 13/50 [00:25<01:10,  1.90s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|â–ˆâ–ˆâ–Š       | 14/50 [00:27<01:06,  1.86s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=43) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [00:28<01:03,  1.82s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=39) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is a key characteristic of the people in the passage?\n",
            "âœ… GT: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them.\n",
            "ðŸ¤– Pred: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [00:30<01:01,  1.80s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=42) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: How does Voldemort appear differently this time?\n",
            "âœ… GT: Unlike before, Voldemort's features are burned and blurred, waxy and oddly distorted, with a permanently bloody look in the eyes, even though the pupils are not yet slits.\n",
            "ðŸ¤– Pred: Voldemort appears to be a very different person than the others, and he appears to be a very different person than the others. He appears to be a very different person than the others, and he appears to be a very different person than the others. He appears to be a very different person than the others. He appears to be a very different person than the others. He appears to be a very different person than the others. He appears to be a very different person than the others. He appears to be a very different person than the others. He appears to be a very different person than the others. He appears to be a very different person than the others. He appears to be a very different person than the others. He appears to be a very different person than the others. He appears to be a very different person than the others. He appears to be a very different person than the others. He appears to be a very different person than the others. He appears to be a very different person than the others. He appears to be a very different person than the others. He appears to be a very different person than the others. He appears to be a very different person than the others. He appears to be a very different person than the others. He appears\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [00:32<00:58,  1.78s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Who is playing poorly at the Quidditch match?\n",
            "âœ… GT: The person currently defending the goalposts is performing poorly.\n",
            "ðŸ¤– Pred: Harry Potter is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is playing poorly at the Quidditch match. He is\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [00:34<00:58,  1.83s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=44) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: How does Snape express his dislike for Harry?\n",
            "âœ… GT: Snape enjoys taking points away from Harry and suggesting his suspension from Hogwarts.\n",
            "ðŸ¤– Pred: Snape expresses his dislike for Harry because he thinks he is a bit too smart to be smart. He thinks he is a bit too smart to be smart. He thinks he is a bit too smart to be smart. He thinks he is a bit too smart to be smart. He thinks he is a bit too smart to be smart. He thinks he is a bit too smart to be smart. He thinks he is a bit too smart to be smart. He thinks he is a bit too smart to be smart. He thinks he is a bit too smart to be smart. He thinks he is a bit too smart to be smart. He thinks he is a bit too smart to be smart. He thinks he is a bit too smart to be smart. He thinks he is a bit too smart to be smart. He thinks he is a bit too smart to be smart. He thinks he is a bit too smart to be smart. He thinks he is a bit too smart to be smart. He thinks he is a bit too smart to be smart. He thinks he is a bit too smart to be smart. He thinks he is a bit too smart to be smart. He thinks he is a bit too smart to be smart. He thinks he is a bit too smart to\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [00:36<00:59,  1.93s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=44) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is the speaker planning to do with the Horcrux?\n",
            "âœ… GT: They plan to destroy the Horcrux as soon as they can.\n",
            "ðŸ¤– Pred: Harry Potter is planning to attend Horcrux, but he is planning to attend Horcrux. He is planning to attend Horcrux, but he is planning to attend Horcrux. He is planning to attend Horcrux, but he is planning to attend Horcrux. He is planning to attend Horcrux, but he is planning to attend Horcrux. He is planning to attend Horcrux, but he is planning to attend Horcrux. He is planning to attend Horcrux, but he is planning to attend Horcrux. He is planning to attend Horcrux, but he is planning to attend Horcrux. He is planning to attend Horcrux, but he is planning to attend Horcrux. He is planning to attend Horcrux, but he is planning to attend Horcrux. He is planning to attend Horcrux,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [00:38<00:56,  1.88s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=43) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What effect does Harry's action have on the marble headstone?\n",
            "âœ… GT: The marble headstone cracks as Harry hides behind it during the attack.\n",
            "ðŸ¤– Pred: Harry's action increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases the marble headstone by 50% and increases\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [00:39<00:53,  1.84s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is a key characteristic of the people in the passage?\n",
            "âœ… GT: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them.\n",
            "ðŸ¤– Pred: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [00:41<00:50,  1.82s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=41) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [00:43<00:48,  1.80s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=45) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is Dean doing when he enters the room?\n",
            "âœ… GT: Dean enters the room and swears loudly upon seeing something.\n",
            "ðŸ¤– Pred: Dean enters the room and enters the room. He enters the room and enters the room. He enters the room and enters the room. He enters the room and enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room. He enters the room.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [00:45<00:46,  1.78s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=41) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What impact does the weather have on the overall atmosphere of the scene?\n",
            "âœ… GT: The poor visibility and dimming light create an ominous and foreboding atmosphere, heightening the tension and suspense leading up to the Quidditch match.\n",
            "ðŸ¤– Pred: The weather is expected to be milder than expected, and the weather is expected to be milder than expected. The weather is expected to be milder than expected, and the weather is expected to be milder than expected. The weather is expected to be milder than expected, and the weather is expected to be milder than expected. The weather is expected to be milder than expected, and the weather is expected to be milder than expected. The weather is expected to be milder than expected, and the weather is expected to be milder than expected. The weather is expected to be milder than expected, and the weather is expected to be milder than expected. The weather is expected to be milder than expected, and the weather is expected to be milder than expected. The weather is expected to be milder than expected, and the weather is expected to be milder than expected. The weather is expected to be milder than expected, and the weather is expected to be milder than expected. The weather is expected to be milder than expected, and the weather is expected to be milder than expected. The weather is expected to be milder than expected, and the weather is expected to be milder than expected. The weather is\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [00:47<00:47,  1.89s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=41) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: How does Lupin react when Harry mentions Black?\n",
            "âœ… GT: Lupin seems reluctant to discuss Black, as evidenced by Harry's observation that 'Lupin was obviously not keen on the subject.'\n",
            "ðŸ¤– Pred: Lupin reacts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [00:49<00:45,  1.89s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=42) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What effect does Hermione's behavior have on Harry?\n",
            "âœ… GT: Harry finds Hermione's behavior during questioning to be off-putting, making it difficult for him to focus on the conversation.\n",
            "ðŸ¤– Pred: Hermione's behavior is influenced by her parents' beliefs about her abilities and abilities. She is influenced by her parents' beliefs about her abilities and abilities. She is influenced by her parents' beliefs about her abilities and abilities. She is influenced by her parents' beliefs about her abilities and abilities. She is influenced by her parents' beliefs about her abilities and abilities. She is influenced by her parents' beliefs about her abilities and abilities. She is influenced by her parents' beliefs about her abilities and abilities. She is influenced by her parents' beliefs about her abilities and abilities. She is influenced by her parents' beliefs about her abilities and abilities. She is influenced by her parents' beliefs about her abilities and abilities. She is influenced by her parents' beliefs about her abilities and abilities. She is influenced by her parents' beliefs about her abilities and abilities. She is influenced by her parents' beliefs about her abilities and abilities. She is influenced by her parents' beliefs about her abilities and abilities. She is influenced by her parents' beliefs about her abilities and abilities. She is influenced by her parents' beliefs about her abilities and abilities. She is influenced by her parents' beliefs about her abilities and abilities. She is influenced by her parents' beliefs about her abilities and abilities. She is\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [00:51<00:42,  1.86s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=44) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is the reward for the student who performs best?\n",
            "âœ… GT: The winning student will receive a small hedgehog named Felix as a prize.\n",
            "ðŸ¤– Pred: The student who performs best is rewarded by the student who performs best. The student who performs best is rewarded by the student who performs best. The student who performs best is rewarded by the student who performs best. The student who performs best is rewarded by the student who performs best. The student who performs best is rewarded by the student who performs best. The student who performs best is rewarded by the student who performs best. The student who performs best is rewarded by the student who performs best. The student who performs best is rewarded by the student who performs best. The student who performs best is rewarded by the student who performs best. The student who performs best is rewarded by the student who performs best. The student who performs best is rewarded by the student who performs best. The student who performs best is rewarded by the student who performs best. The student who performs best is rewarded by the student who performs best. The student who performs best is rewarded by the student who performs best. The student who performs best is rewarded by the student who performs best. The student who performs best is rewarded by the student who performs best. The student who performs best is rewarded by the student who performs best. The student who performs best is rewarded by the student who performs best. The student who performs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [00:52<00:40,  1.83s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=46) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What motivates Harry to sit down and listen to the others?\n",
            "âœ… GT: Although Harry is still angry, his need for information drives him to listen to what the others have to say.\n",
            "ðŸ¤– Pred: The motivation of Harry to sit down and listen to the others is to listen to the others. The motivation of Harry to sit down and listen to the others is to listen to the others. The motivation of Harry to sit down and listen to the others is to listen to the others. The motivation of Harry to sit down and listen to the others is to listen to the others. The motivation of Harry to sit down and listen to the others is to listen to the others. The motivation of Harry to sit down and listen to the others is to listen to the others. The motivation of Harry to sit down and listen to the others is to listen to the others. The motivation of Harry to sit down and listen to the others is to listen to the others. The motivation of Harry to sit down and listen to the others is to listen to the others. The motivation of Harry to sit down and listen to the others is to listen to the others. The motivation of Harry to sit down and listen to the others is to listen to the others. The motivation of Harry to sit down and listen to the others is to listen to the others. The motivation of Harry to sit down and listen to the others is to listen to the others. The motivation of Harry to sit down and listen\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [00:54<00:37,  1.80s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What kinds of negative effects can come from reading certain books according to Ron?\n",
            "âœ… GT: Ron mentions that some books his father's friends have read had negative consequences such as burning out eyes or making readers speak in limericks for the rest of their lives.\n",
            "ðŸ¤– Pred: The negative effects can come from reading certain books according to Ron's knowledge of the subject. The negative effects can come from reading certain books according to Ron's knowledge of the subject. The negative effects can come from reading certain books according to Ron's knowledge of the subject. The negative effects can come from reading certain books according to Ron's knowledge of the subject. The negative effects can come from reading certain books according to Ron's knowledge of the subject. The negative effects can come from reading certain books according to Ron's knowledge of the subject. The negative effects can come from reading certain books according to Ron's knowledge of the subject. The negative effects can come from reading certain books according to Ron's knowledge of the subject. The negative effects can come from reading certain books according to Ron's knowledge of the subject. The negative effects can come from reading certain books according to Ron's knowledge of the subject. The negative effects can come from reading certain books according to Ron's knowledge of the subject. The negative effects can come from reading certain books according to Ron's knowledge of the subject. The negative effects can come from reading certain books according to Ron's knowledge of the subject. The negative effects can come from reading certain books according to Ron's knowledge of the subject. The negative effects can\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [00:56<00:35,  1.78s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=52) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: How does Harry react to the potential danger?\n",
            "âœ… GT: Harry is concerned and believes that it's worth investigating, despite Ron's objections.\n",
            "ðŸ¤– Pred: Harry reacts to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the potential danger by reacting to the\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [00:58<00:34,  1.79s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=41) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What was Mr. and Mrs. Dursley's reason for not wanting owls to follow them?\n",
            "âœ… GT: They were on a secret mission and did not want unwanted attention. Dementors also accompanied the owls, adding to their concern.\n",
            "ðŸ¤– Pred: They wanted owls to follow them because they were afraid of being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked by the owls. They wanted to avoid being attacked\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [01:00<00:34,  1.93s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=42) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why can't Hermione move despite Harry's urging?\n",
            "âœ… GT: Hermione is frozen with fear and unable to move, despite Harry's attempts to pull her towards safety.\n",
            "ðŸ¤– Pred: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [01:02<00:31,  1.88s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=43) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is Dumbledore's responsibility towards Harry regarding Hagrid?\n",
            "âœ… GT: Dumbledore has a duty to warn Harry and his fellow students about the potential dangers of associating with Hagrid due to his friendship with the boy who brought down Voldemort.\n",
            "ðŸ¤– Pred: Dumbledore is responsible for the Harry Potter trivia question, which is about Hagrid's relationship with the Lord Voldemort. He is responsible for the Harry Potter trivia question, which is about Hagrid's relationship with the Lord Voldemort. He is responsible for the Harry Potter trivia question, which is about Hagrid's relationship with the Lord Voldemort. He is responsible for the Harry Potter trivia question, which is about Hagrid's relationship with the Lord Voldemort. He is responsible for the Harry Potter trivia question, which is about Hagrid's relationship with the Lord Voldemort. He is responsible for the Harry Potter trivia question, which is about Hagrid's relationship with the Lord Voldemort. He is responsible for the Harry Potter trivia question, which is about Hagrid's relationship with the Lord Voldemort. He is responsible for the Harry Potter trivia question, which is about Hagrid's relationship with the Lord Voldemort. He is responsible for the Harry Potter trivia question, which is about Hagrid's relationship with the Lord Voldemort. He is responsible for the Harry Potter trivia question, which is about Hagrid's relationship with the Lord Voldemort. He is responsible for the Harry Potter trivia question, which is about Hagrid's relationship with the Lord Voldemort. He is responsible for the Harry Potter trivia question, which is about Hag\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [01:03<00:29,  1.83s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is a key characteristic of the people in the passage?\n",
            "âœ… GT: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them.\n",
            "ðŸ¤– Pred: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [01:05<00:27,  1.81s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=43) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [01:07<00:25,  1.80s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=43) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Who is sitting next to Hermione when she enters the room?\n",
            "âœ… GT: Professor Umbridge is sitting next to Hermione when she enters the room.\n",
            "ðŸ¤– Pred: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [01:09<00:23,  1.79s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is a key characteristic of the people in the passage?\n",
            "âœ… GT: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them.\n",
            "ðŸ¤– Pred: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [01:11<00:22,  1.88s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=43) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: How does the shield affect Harry and Ron?\n",
            "âœ… GT: The shield forces Harry and Ron backwards a few steps and makes them glare at each other through it, indicating a deep rift between them.\n",
            "ðŸ¤– Pred: The shield affects Harry and Ron by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield's strength by increasing the shield\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [01:13<00:21,  1.92s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=39) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is a key characteristic of the people in the passage?\n",
            "âœ… GT: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them.\n",
            "ðŸ¤– Pred: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [01:15<00:18,  1.88s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is the weather like during breakfast?\n",
            "âœ… GT: A clear, pale blue sky is seen through the Great Hall ceiling, indicating good weather.\n",
            "ðŸ¤– Pred: The weather is warm and humid, and the weather is warm. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and humid. The weather is warm and\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [01:16<00:16,  1.84s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=43) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [01:18<00:14,  1.80s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=38) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is a key characteristic of the people in the passage?\n",
            "âœ… GT: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them.\n",
            "ðŸ¤– Pred: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [01:20<00:12,  1.79s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=44) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What condition is the owl in?\n",
            "âœ… GT: The owl lies motionless and pathetic on the floor of its cage. Harry cannot understand what has happened to it.\n",
            "ðŸ¤– Pred: The owl is a small, small, and small owl with a small, small, and small, small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small, and small,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [01:22<00:10,  1.78s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=41) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: According to Bill, what is the danger level of these people?\n",
            "âœ… GT: Bill believes that some of these people are really dangerous.\n",
            "ðŸ¤– Pred: They are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because they are dangerous because\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [01:24<00:09,  1.94s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=43) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What did Professor Lupin promise to teach Harry?\n",
            "âœ… GT: Professor Lupin promised to teach Harry how to ward off Dementors during the coming week.\n",
            "ðŸ¤– Pred: Professor Lupin promised to teach Harry to be a wizard, and he promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to teach Harry to be a wizard. He promised to\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [01:26<00:07,  1.88s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=43) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is a key characteristic of the people in the passage?\n",
            "âœ… GT: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them.\n",
            "ðŸ¤– Pred: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [01:27<00:05,  1.83s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=45) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is a key characteristic of the people in the passage?\n",
            "âœ… GT: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them.\n",
            "ðŸ¤– Pred: Mr. and Mrs. Dursley are described as proud to be normal and ordinary. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to them. They avoid anything that might seem strange or mysterious to\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [01:29<00:03,  1.80s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: What is Mrs. Weasley's motivation for confronting Mundungus Fletcher?\n",
            "âœ… GT: Mrs. Weasley wants to confront Mundungus Fletcher because he failed to follow Harry as instructed. She has also been eager to have a go at him since the incident.\n",
            "ðŸ¤– Pred: She is motivated to confront Mundungus Fletcher because she believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundungus Fletcher is a good friend. She believes Mundung\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [01:31<00:01,  1.78s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: Why does the author emphasize a certain aspect?\n",
            "âœ… GT: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life.\n",
            "ðŸ¤– Pred: The author emphasizes their normality to highlight a contrast with the magical events that are about to unfold. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are compared to the Dursleys' everyday life. It sets the stage for showing how unusual the upcoming events are\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:32<00:00,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“˜ Q: How did Ginny Weasley react to her diary?\n",
            "âœ… GT: Ginny Weasley initially trusted her diary but later became suspicious and tried to dispose of it.\n",
            "ðŸ¤– Pred: Ginny Weasley reacted to her diary by saying: \"I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she meant by that. I don't know what she\n",
            "\n",
            "ðŸŽ¯ Exact Match: 0.00%\n",
            "ðŸŽ¯ Average F1: 11.84%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bert-score evaluate\n",
        "!pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nv0mQ4SGRCHw",
        "outputId": "1ef7d0fb-ebd7-466e-c7f0-028332008029"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.1.1 sacrebleu-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "\n",
        "# Imports\n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "\n",
        "# Load dataset\n",
        "train_dataset = load_dataset(\"vapit/HarryPotterQA\")['train']\n",
        "validation_dataset = load_dataset(\"vapit/HarryPotterQA\")['validation'].select(range(50))\n",
        "\n",
        "print(f\"Train split: {len(train_dataset)} questions\")\n",
        "print(f\"Validation split: {len(validation_dataset)} questions\")\n",
        "\n",
        "# Load model on GPU\n",
        "model_name = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
        "\n",
        "# Load evaluation metrics\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "bleu = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "# Generate predictions and collect references\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "print(\"Generating responses...\")\n",
        "for sample in tqdm(validation_dataset):\n",
        "    question = sample['question']\n",
        "    reference = sample['answer']\n",
        "\n",
        "    prompt = f\"Q: {question}\\nA:\"\n",
        "    output = generator(prompt, max_length=100, num_return_sequences=1, do_sample=True, temperature=0.7)[0]['generated_text']\n",
        "\n",
        "    answer_start = output.find(\"A:\") + 2\n",
        "    answer = output[answer_start:].strip().split('\\n')[0]\n",
        "\n",
        "    predictions.append(answer)\n",
        "    references.append(reference)\n",
        "\n",
        "# Evaluate BLEU (CPU is sufficient)\n",
        "bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
        "print(f\"\\nBLEU Score: {bleu_score['score']:.2f}\")\n",
        "\n",
        "# Evaluate BERTScore (GPU-accelerated)\n",
        "print(\"Computing BERTScore...\")\n",
        "bertscore_result = bertscore.compute(predictions=predictions,\n",
        "                                     references=references,\n",
        "                                     lang=\"en\",\n",
        "                                     device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "avg_f1 = sum(bertscore_result['f1']) / len(bertscore_result['f1'])\n",
        "print(f\"BERTScore (F1): {avg_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9d90cc56e14f4e5abbe3400984294472",
            "4d04c21803f44aaf9e318d6f3fa462fc",
            "f2223bf456524295968bafd877c826bd",
            "405efbc110084787b0b6328323f7cc43",
            "5a0b2316c73741f3ae8297a8935d7381",
            "87463d26929545d6b9245a3012d55542",
            "c12354e1f9aa40cf9bdf387e8a2788a5",
            "9b93f8c6994c4e04a6ec80e6a862af47",
            "98b834b286544854901ff97da9d9ec78",
            "972494fc2f90456786c7c88c97c3b0d3",
            "683fe91b5ccc4045b952163429e7e358",
            "6fa362faa4eb48cb9bc84bd844c0b189",
            "d35fe5e2d12148eca09b38cde4ca71b8",
            "91d1531074574c5590fa946b7f3b600a",
            "5b41ca91021b45d2b3256501af713287",
            "67c896a60cc74b15a8f014ef40b4cd69",
            "107e4bd5150f4a75a02bc51590042c29",
            "6d8fea7a2eb84585ab540823f760663b",
            "057742f893a445ac9e31ed4600d78645",
            "cdb8c49edb8a49e8b062f9fd0014e5c2",
            "fc667b7cfab94afcace71b18842751da",
            "797d01cd2aab4586a6b629b5895d76ee",
            "5d8bc9c15ea449b7a0411e5486b9fa3c",
            "3135d45ec778461c972e520fc3f71479",
            "89a05d73746e47759af05940076abc4d",
            "152900b3214e4aeca8151118826251ae",
            "b837e4f225df4bb8906e3c3b6535fb70",
            "78b62b2e2dcd40ac99109c090f1fd316",
            "b20dbf4a1987425da4af87f3869510a2",
            "392bef8628284a5aac1e093bab7c5b01",
            "28732882135149feb5d7f38100c5718e",
            "bdb59ae5a46d485ab32068dceba11db9",
            "8bd8dede9c5b4ee7840f8f2006c15531",
            "04ae56ad2db94b6f90aa74af9212f43d",
            "44a126e9e7f84dc2a8273c68d04d34dc",
            "d0575dd6f84449e797459cebdd4b7c4a",
            "74505e3ed6ed4aa5bb98f2b552085e25",
            "aa35d6eed25d448c8d70f89b3fa71e6a",
            "3c5b79e4edce4f6ebbac9285e5f54d82",
            "d219f44f176a4b0d84407d445889c513",
            "7e2da9393bab431aa8a6ec88538df994",
            "aaf15bb4af0249ed992d9d9767dc718f",
            "fb99140a3c2b456bb82966bacb3ddd19",
            "6df114b593fd425daaf17565040c74b8",
            "3e10202b2eda4c1388d0e4380884211e",
            "13bf210fd39649ba90541ccdf0218f9f",
            "ae832281452d415aa35e3990ecf9354c",
            "7db345d6f43144129f1b08ee38c389a5",
            "d2ed695e503f4a5d9d29763e361d8f79",
            "d8c2f3aa3d1e4488af132dbe2d78e4ff",
            "926988b57c7d4f8b9110b7c08f1fb21e",
            "8ec08fdb4cab4cc98417270a686a59e2",
            "4f8d258000004bbea0f07ba5cf163d83",
            "3938bb79f1de4090b25a00b9fd84e6ea",
            "0abe2f0df0d4477cbda5a5259d38dddb",
            "573830a41e594dda9f833e5c91c217a1",
            "c5ff2301ae884fafab2382adbb2af9e6",
            "9c53857f43d34fa58d3acc77566b58c4",
            "161faf0efcb34b6b8349d639eb0c2104",
            "b3047e31a77a45a5903625b77d8a5088",
            "d89b03646b0b402594e5679d2de7da4f",
            "e50583961eaf4995b1f443137ef575db",
            "3fd5f8fd1ea04258bbf5b93cd4e5aed2",
            "659c9ffcbbc44ce486878c455f3b3e86",
            "a64f20739306460e872b56f3ccbdd114",
            "82cd2461d1e94ef19d15b4857c174014"
          ]
        },
        "id": "wtHTsWzJRFq_",
        "outputId": "ddfd91b0-6454-4978-8aca-3004cdbf8c52"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train split: 25999 questions\n",
            "Validation split: 50 questions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "  2%|â–         | 1/50 [00:01<01:16,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "  4%|â–         | 2/50 [00:03<01:14,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "  6%|â–Œ         | 3/50 [00:04<01:13,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "  8%|â–Š         | 4/50 [00:06<01:15,  1.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 10%|â–ˆ         | 5/50 [00:08<01:18,  1.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 12%|â–ˆâ–        | 6/50 [00:09<01:14,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 14%|â–ˆâ–        | 7/50 [00:11<01:10,  1.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 16%|â–ˆâ–Œ        | 8/50 [00:13<01:07,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 18%|â–ˆâ–Š        | 9/50 [00:14<01:05,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 20%|â–ˆâ–ˆ        | 10/50 [00:16<01:03,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 22%|â–ˆâ–ˆâ–       | 11/50 [00:17<01:00,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 24%|â–ˆâ–ˆâ–       | 12/50 [00:19<01:06,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 26%|â–ˆâ–ˆâ–Œ       | 13/50 [00:21<01:02,  1.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 28%|â–ˆâ–ˆâ–Š       | 14/50 [00:23<00:59,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [00:24<00:57,  1.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [00:26<00:54,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [00:27<00:52,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [00:29<00:50,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [00:31<00:52,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [00:32<00:51,  1.72s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [00:34<00:48,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [00:36<00:45,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [00:37<00:43,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [00:39<00:41,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [00:40<00:39,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [00:42<00:38,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [00:44<00:39,  1.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [00:45<00:36,  1.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [00:47<00:34,  1.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [00:49<00:32,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [00:50<00:30,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [00:52<00:28,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [00:53<00:26,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [00:55<00:27,  1.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [00:57<00:25,  1.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [00:58<00:23,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [00:59<00:17,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [01:01<00:16,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [01:03<00:18,  1.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [01:05<00:19,  1.91s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [01:07<00:15,  1.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [01:11<00:19,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [01:13<00:17,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [01:16<00:15,  2.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [01:19<00:13,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [01:24<00:12,  3.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [01:28<00:10,  3.52s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [01:35<00:08,  4.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [01:39<00:04,  4.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:42<00:00,  2.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BLEU Score: 0.25\n",
            "Computing BERTScore...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d90cc56e14f4e5abbe3400984294472"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6fa362faa4eb48cb9bc84bd844c0b189"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d8bc9c15ea449b7a0411e5486b9fa3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04ae56ad2db94b6f90aa74af9212f43d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e10202b2eda4c1388d0e4380884211e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "573830a41e594dda9f833e5c91c217a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERTScore (F1): 0.8357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(base, \"./peft-gpt2-hp/checkpoint-19500\")\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "bleu = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "# Generate predictions and collect references\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "print(\"Generating responses...\")\n",
        "for sample in tqdm(validation_dataset):\n",
        "    question = sample['question']\n",
        "    reference = sample['answer']\n",
        "\n",
        "    prompt = f\"Q: {question}\\nA:\"\n",
        "    output = generator(prompt, max_length=100, num_return_sequences=1, do_sample=True, temperature=0.7)[0]['generated_text']\n",
        "\n",
        "    answer_start = output.find(\"A:\") + 2\n",
        "    answer = output[answer_start:].strip().split('\\n')[0]\n",
        "\n",
        "    predictions.append(answer)\n",
        "    references.append(reference)\n",
        "\n",
        "# Evaluate BLEU (CPU is sufficient)\n",
        "bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
        "print(f\"\\nBLEU Score: {bleu_score['score']:.2f}\")\n",
        "\n",
        "# Evaluate BERTScore (GPU-accelerated)\n",
        "print(\"Computing BERTScore...\")\n",
        "bertscore_result = bertscore.compute(predictions=predictions,\n",
        "                                     references=references,\n",
        "                                     lang=\"en\",\n",
        "                                     device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "avg_f1 = sum(bertscore_result['f1']) / len(bertscore_result['f1'])\n",
        "print(f\"BERTScore (F1): {avg_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PdkOCfVTaRl",
        "outputId": "d02b2f49-9216-4db2-cff4-cb81a273d21b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "  2%|â–         | 1/50 [00:01<01:30,  1.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "  4%|â–         | 2/50 [00:04<01:44,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "  6%|â–Œ         | 3/50 [00:06<01:35,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "  8%|â–Š         | 4/50 [00:07<01:29,  1.95s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 10%|â–ˆ         | 5/50 [00:09<01:25,  1.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 12%|â–ˆâ–        | 6/50 [00:11<01:22,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 14%|â–ˆâ–        | 7/50 [00:13<01:19,  1.84s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 16%|â–ˆâ–Œ        | 8/50 [00:15<01:21,  1.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 18%|â–ˆâ–Š        | 9/50 [00:17<01:22,  2.00s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 20%|â–ˆâ–ˆ        | 10/50 [00:19<01:17,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 22%|â–ˆâ–ˆâ–       | 11/50 [00:21<01:14,  1.91s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 24%|â–ˆâ–ˆâ–       | 12/50 [00:23<01:11,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 26%|â–ˆâ–ˆâ–Œ       | 13/50 [00:24<01:08,  1.86s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 28%|â–ˆâ–ˆâ–Š       | 14/50 [00:26<01:07,  1.89s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [00:32<01:40,  2.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [00:33<01:27,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [00:35<01:17,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [00:37<01:09,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [00:39<01:06,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [00:41<01:05,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [00:43<01:00,  2.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [00:45<00:55,  2.00s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [00:47<00:52,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [00:49<00:49,  1.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [00:50<00:46,  1.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [00:53<00:49,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [00:55<00:45,  1.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [00:56<00:42,  1.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [00:58<00:39,  1.89s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [01:00<00:37,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [01:02<00:35,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [01:04<00:35,  1.97s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [01:06<00:33,  1.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [01:08<00:31,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [01:10<00:28,  1.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [01:12<00:26,  1.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [01:13<00:24,  1.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [01:15<00:22,  1.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [01:18<00:22,  2.01s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [01:20<00:19,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [01:21<00:17,  1.91s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [01:23<00:15,  1.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [01:25<00:13,  1.86s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [01:27<00:11,  1.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [01:29<00:10,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [01:31<00:07,  1.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [01:33<00:05,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [01:35<00:03,  1.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [01:37<00:01,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:38<00:00,  1.98s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BLEU Score: 5.91\n",
            "Computing BERTScore...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERTScore (F1): 0.8589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Imports\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"vapit/HarryPotterQA\")\n",
        "train_dataset = dataset[\"train\"]\n",
        "validation_dataset = dataset[\"validation\"].select(range(50))  # limit to 50 samples\n",
        "\n",
        "print(f\"Using {len(train_dataset)} training answers as retrieval database.\")\n",
        "print(f\"Evaluating on {len(validation_dataset)} validation samples.\")\n",
        "\n",
        "# Load sentence embedding model\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Embed training answers (contexts)\n",
        "train_answers = train_dataset[\"answer\"]\n",
        "train_embeddings = embedder.encode(train_answers, convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "# Load generator model\n",
        "gen_model = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(gen_model)\n",
        "model = AutoModelForCausalLM.from_pretrained(gen_model).to(\"cuda\")\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
        "\n",
        "# Evaluate on 50 validation samples\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "print(\"Generating RAG-style answers...\")\n",
        "for sample in tqdm(validation_dataset):\n",
        "    question = sample[\"question\"]\n",
        "    reference = sample[\"answer\"]\n",
        "\n",
        "    # Embed question\n",
        "    question_embedding = embedder.encode(question, convert_to_tensor=True)\n",
        "\n",
        "    # Retrieve top-k similar answers from training set\n",
        "    cosine_scores = util.pytorch_cos_sim(question_embedding, train_embeddings)[0]\n",
        "    top_k = torch.topk(cosine_scores, k=3)  # Top-3 retrieved answers\n",
        "    retrieved_contexts = [train_answers[idx] for idx in top_k.indices]\n",
        "\n",
        "    # Concatenate context + question\n",
        "    context = \"\\n\".join(retrieved_contexts)\n",
        "    prompt = f\"Context:\\n{context}\\n\\nQ: {question}\\nA:\"\n",
        "\n",
        "    # Generate answer\n",
        "    output = generator(prompt, max_length=150, do_sample=True, temperature=0.7, num_return_sequences=1)[0]['generated_text']\n",
        "    generated_answer = output.split(\"A:\")[-1].strip().split('\\n')[0]\n",
        "\n",
        "    predictions.append(generated_answer)\n",
        "    references.append(reference)\n",
        "\n",
        "# Load evaluation metrics\n",
        "bleu = evaluate.load(\"sacrebleu\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "# BLEU Score\n",
        "bleu_score = bleu.compute(predictions=predictions, references=[[r] for r in references])\n",
        "print(f\"\\nBLEU Score (manual RAG, 50 samples): {bleu_score['score']:.2f}\")\n",
        "\n",
        "# BERTScore\n",
        "print(\"Computing BERTScore...\")\n",
        "bertscore_result = bertscore.compute(predictions=predictions,\n",
        "                                     references=references,\n",
        "                                     lang=\"en\",\n",
        "                                     device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "avg_f1 = sum(bertscore_result['f1']) / len(bertscore_result['f1'])\n",
        "print(f\"BERTScore (F1, manual RAG): {avg_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "63a1f5ccebb1456ca9790ac52feb25e2",
            "d51b619c5d584f5d9d264f1ee1cf16a1",
            "e8f33c3f955b4362a88e59229f82e852",
            "1f41742143c8408a99e8f29d66ccef39",
            "1291e449cf5649a1a05e9d1025f04c2b",
            "331721b4bf184782b3141cc3c3a46223",
            "3d4117121d2f43f991c0085373741fbf",
            "d980cc1d5eec4eb1994565409ee70013",
            "f032c63df2a24961b7530f554194f7af",
            "7c3afbc1257048128682422d0dc68148",
            "8ca66032d3c541e58bfc385038fc9be8"
          ]
        },
        "id": "rSXjWGnBUOQF",
        "outputId": "2a3e6473-bfa0-4464-c4e7-96bf90a86d68"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 25999 training answers as retrieval database.\n",
            "Evaluating on 50 validation samples.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/813 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63a1f5ccebb1456ca9790ac52feb25e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating RAG-style answers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "  2%|â–         | 1/50 [00:01<01:19,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "  4%|â–         | 2/50 [00:03<01:16,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "  6%|â–Œ         | 3/50 [00:04<01:14,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "  8%|â–Š         | 4/50 [00:06<01:15,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 10%|â–ˆ         | 5/50 [00:08<01:19,  1.77s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 12%|â–ˆâ–        | 6/50 [00:10<01:14,  1.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 14%|â–ˆâ–        | 7/50 [00:11<01:11,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 16%|â–ˆâ–Œ        | 8/50 [00:13<01:08,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 18%|â–ˆâ–Š        | 9/50 [00:14<01:06,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 20%|â–ˆâ–ˆ        | 10/50 [00:16<01:04,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 22%|â–ˆâ–ˆâ–       | 11/50 [00:17<01:01,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 24%|â–ˆâ–ˆâ–       | 12/50 [00:20<01:06,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 26%|â–ˆâ–ˆâ–Œ       | 13/50 [00:21<01:03,  1.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 28%|â–ˆâ–ˆâ–Š       | 14/50 [00:23<00:59,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [00:24<00:57,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [00:26<00:54,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [00:27<00:52,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [00:29<00:50,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [00:31<00:52,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [00:33<00:51,  1.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [00:34<00:48,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [00:36<00:46,  1.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [00:37<00:43,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [00:39<00:41,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [00:41<00:39,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [00:42<00:39,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [00:44<00:40,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [00:46<00:37,  1.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [00:47<00:34,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [00:49<00:32,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [00:51<00:30,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [00:52<00:28,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [00:54<00:26,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [00:56<00:27,  1.72s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [00:57<00:25,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [00:59<00:23,  1.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [01:00<00:21,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [01:02<00:19,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [01:04<00:17,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [01:05<00:15,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [01:07<00:14,  1.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [01:09<00:13,  1.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [01:10<00:11,  1.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [01:12<00:09,  1.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [01:13<00:08,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [01:15<00:06,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [01:17<00:04,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [01:18<00:03,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [01:20<00:01,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:22<00:00,  1.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BLEU Score (manual RAG, 50 samples): 0.36\n",
            "Computing BERTScore...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERTScore (F1, manual RAG): 0.8091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2ab1dab12f584b5693d05b8781a1d4e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8fa625d14ea4e929856fd6def372e6e",
              "IPY_MODEL_5466cebd94d44d4f9b039ccfb0e9f36d",
              "IPY_MODEL_ef0a8acdcdf64286838a4fdb7f5d0510"
            ],
            "layout": "IPY_MODEL_6ec0261c6c214660b3c4818b273a2516"
          }
        },
        "e8fa625d14ea4e929856fd6def372e6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad7d7be1461841c8af4fc5b7d7dab773",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2ca64ccb15434da69f2aa94c0fcc096f",
            "value": "Downloadingâ€‡readme:â€‡100%"
          }
        },
        "5466cebd94d44d4f9b039ccfb0e9f36d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_079c763bd67b405a9cd203d9fec9fbfa",
            "max": 2340,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d16ffe8db5e4ea784736366791c9e2f",
            "value": 2340
          }
        },
        "ef0a8acdcdf64286838a4fdb7f5d0510": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f40ed10a208743a993f67910d6c0dacb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1f0d4716c5e14db1bfb87b249eecad4e",
            "value": "â€‡2.34k/2.34kâ€‡[00:00&lt;00:00,â€‡41.5kB/s]"
          }
        },
        "6ec0261c6c214660b3c4818b273a2516": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad7d7be1461841c8af4fc5b7d7dab773": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ca64ccb15434da69f2aa94c0fcc096f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "079c763bd67b405a9cd203d9fec9fbfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d16ffe8db5e4ea784736366791c9e2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f40ed10a208743a993f67910d6c0dacb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f0d4716c5e14db1bfb87b249eecad4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f6d2b01a72f4aa09ab90370e202041d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f71e742abeb41a98ffef91e31a43508",
              "IPY_MODEL_969ee8d23408406aa1bdcdeef8a90823",
              "IPY_MODEL_df280ff919c84adfb4f95ae6d7111eca"
            ],
            "layout": "IPY_MODEL_8f44fd49800544c89d73ea4ebe2c1246"
          }
        },
        "1f71e742abeb41a98ffef91e31a43508": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5fceee6efce43f8a62588f383b7143c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e53be6d999624e93acbf5e6448ff26a5",
            "value": "Downloadingâ€‡dataâ€‡files:â€‡100%"
          }
        },
        "969ee8d23408406aa1bdcdeef8a90823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6aa63d6e98e49f48fbbcef7a3397206",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5194694c1d6a45f384987d4f6c47d06f",
            "value": 2
          }
        },
        "df280ff919c84adfb4f95ae6d7111eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_123196b441fe4d138e43558a4f42dea7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4672a59ed3e04d1b8b9d9e6f98353c63",
            "value": "â€‡2/2â€‡[00:00&lt;00:00,â€‡â€‡2.68it/s]"
          }
        },
        "8f44fd49800544c89d73ea4ebe2c1246": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5fceee6efce43f8a62588f383b7143c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e53be6d999624e93acbf5e6448ff26a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6aa63d6e98e49f48fbbcef7a3397206": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5194694c1d6a45f384987d4f6c47d06f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "123196b441fe4d138e43558a4f42dea7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4672a59ed3e04d1b8b9d9e6f98353c63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e09059fc8bfa46358223263199c03a49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6c250958a5e4ef7ac8706377c9940bd",
              "IPY_MODEL_ec79967fff9a43868fcf21aebba88444",
              "IPY_MODEL_aac42f094c0f4baa9f011f8be29c55c8"
            ],
            "layout": "IPY_MODEL_63cdf3ac081043618dcffa9486b398a1"
          }
        },
        "c6c250958a5e4ef7ac8706377c9940bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cd3521cc9d14b7ab57a4c801ba8d5f9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fccb433b106c466db0793daf68e8940d",
            "value": "Extractingâ€‡dataâ€‡files:â€‡100%"
          }
        },
        "ec79967fff9a43868fcf21aebba88444": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad651ac8ae4a4e3080fc16b3816208b8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d311b49b946c485f81c189d26e4a9d8a",
            "value": 2
          }
        },
        "aac42f094c0f4baa9f011f8be29c55c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62ecd46d06b94eb791e29b404a8f0e9d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c604bdb4f23a46749545a2a911296f14",
            "value": "â€‡2/2â€‡[00:00&lt;00:00,â€‡38.66it/s]"
          }
        },
        "63cdf3ac081043618dcffa9486b398a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cd3521cc9d14b7ab57a4c801ba8d5f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fccb433b106c466db0793daf68e8940d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad651ac8ae4a4e3080fc16b3816208b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d311b49b946c485f81c189d26e4a9d8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62ecd46d06b94eb791e29b404a8f0e9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c604bdb4f23a46749545a2a911296f14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd62427eb68b4b66ba130db0c7b0add5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f615e3a975ca43a9a9ddfe98128c9345",
              "IPY_MODEL_78b201eebc3644a5b7a6b4e8c4411798",
              "IPY_MODEL_5cebf8fce96d4b8bb7e425ea7826c613"
            ],
            "layout": "IPY_MODEL_e112a1f874664a27b1867bc8c9c8f120"
          }
        },
        "f615e3a975ca43a9a9ddfe98128c9345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f08e7c719a7481aa7cec8e468ffb883",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_12c4b617eca04c6cb605411d2013de3b",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "78b201eebc3644a5b7a6b4e8c4411798": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_124b971d3c2742e3b9e90902e4fb6c9c",
            "max": 25999,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d2fe72726edc4687880198de3d20d341",
            "value": 25999
          }
        },
        "5cebf8fce96d4b8bb7e425ea7826c613": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de6fc6473b564369a55ebbe9bd4dbfcb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4fa4dae32ceb42bba3f538b5c915b1be",
            "value": "â€‡25999/25999â€‡[00:00&lt;00:00,â€‡54824.64â€‡examples/s]"
          }
        },
        "e112a1f874664a27b1867bc8c9c8f120": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f08e7c719a7481aa7cec8e468ffb883": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12c4b617eca04c6cb605411d2013de3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "124b971d3c2742e3b9e90902e4fb6c9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2fe72726edc4687880198de3d20d341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de6fc6473b564369a55ebbe9bd4dbfcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fa4dae32ceb42bba3f538b5c915b1be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "734417dd9b9f40f6b32b9d070b33f278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b634886823b4255981b34e63aa66c85",
              "IPY_MODEL_220156c0862d4d46b89a19f5020bb402",
              "IPY_MODEL_3d15c29d4fdf49f29813a8cde8383b30"
            ],
            "layout": "IPY_MODEL_312b9de34e004dd181b907014ab4a39f"
          }
        },
        "3b634886823b4255981b34e63aa66c85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6069d171b17e4b90931d977f43ad4456",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_17aaec6193a2431895220c627e131b3e",
            "value": "Generatingâ€‡validationâ€‡split:â€‡100%"
          }
        },
        "220156c0862d4d46b89a19f5020bb402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9851c0a32e7f44568419abed5aea68ae",
            "max": 2889,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6e7222521fd4e2da7b1c7e29e891e90",
            "value": 2889
          }
        },
        "3d15c29d4fdf49f29813a8cde8383b30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8293e1731a4d4f019b0e14e741c1459c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_af0b3e7089dc4b8d851b6317f8292684",
            "value": "â€‡2889/2889â€‡[00:00&lt;00:00,â€‡71131.21â€‡examples/s]"
          }
        },
        "312b9de34e004dd181b907014ab4a39f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6069d171b17e4b90931d977f43ad4456": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17aaec6193a2431895220c627e131b3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9851c0a32e7f44568419abed5aea68ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6e7222521fd4e2da7b1c7e29e891e90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8293e1731a4d4f019b0e14e741c1459c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af0b3e7089dc4b8d851b6317f8292684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9712dc81b14245e080525a92d46166ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_535601458da045629532ab1b7863a3b0",
              "IPY_MODEL_086ca99a87c54e038b05e9735c8eff58",
              "IPY_MODEL_d333987962664b84a62185394aa73fa8"
            ],
            "layout": "IPY_MODEL_0dfddb661bfc4847bc5ae80147942cf0"
          }
        },
        "535601458da045629532ab1b7863a3b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1bfa15fd300422d92e838b3c1513489",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2cab6d8f5398452ab45b49abfb688329",
            "value": "Map:â€‡100%"
          }
        },
        "086ca99a87c54e038b05e9735c8eff58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e7d658c04dd42bdbf8ed571d70ea6f0",
            "max": 2889,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91193d6bcea74d07bc4f200a7395ba3a",
            "value": 2889
          }
        },
        "d333987962664b84a62185394aa73fa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0689a7ccc54e422288e24fc82b294136",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3269598511e948f9ae623cbe1e4aa6e0",
            "value": "â€‡2889/2889â€‡[00:01&lt;00:00,â€‡1657.27â€‡examples/s]"
          }
        },
        "0dfddb661bfc4847bc5ae80147942cf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1bfa15fd300422d92e838b3c1513489": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cab6d8f5398452ab45b49abfb688329": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e7d658c04dd42bdbf8ed571d70ea6f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91193d6bcea74d07bc4f200a7395ba3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0689a7ccc54e422288e24fc82b294136": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3269598511e948f9ae623cbe1e4aa6e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d90cc56e14f4e5abbe3400984294472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d04c21803f44aaf9e318d6f3fa462fc",
              "IPY_MODEL_f2223bf456524295968bafd877c826bd",
              "IPY_MODEL_405efbc110084787b0b6328323f7cc43"
            ],
            "layout": "IPY_MODEL_5a0b2316c73741f3ae8297a8935d7381"
          }
        },
        "4d04c21803f44aaf9e318d6f3fa462fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87463d26929545d6b9245a3012d55542",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c12354e1f9aa40cf9bdf387e8a2788a5",
            "value": "tokenizer_config.json:â€‡100%"
          }
        },
        "f2223bf456524295968bafd877c826bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b93f8c6994c4e04a6ec80e6a862af47",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98b834b286544854901ff97da9d9ec78",
            "value": 25
          }
        },
        "405efbc110084787b0b6328323f7cc43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_972494fc2f90456786c7c88c97c3b0d3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_683fe91b5ccc4045b952163429e7e358",
            "value": "â€‡25.0/25.0â€‡[00:00&lt;00:00,â€‡882B/s]"
          }
        },
        "5a0b2316c73741f3ae8297a8935d7381": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87463d26929545d6b9245a3012d55542": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c12354e1f9aa40cf9bdf387e8a2788a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b93f8c6994c4e04a6ec80e6a862af47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98b834b286544854901ff97da9d9ec78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "972494fc2f90456786c7c88c97c3b0d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "683fe91b5ccc4045b952163429e7e358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fa362faa4eb48cb9bc84bd844c0b189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d35fe5e2d12148eca09b38cde4ca71b8",
              "IPY_MODEL_91d1531074574c5590fa946b7f3b600a",
              "IPY_MODEL_5b41ca91021b45d2b3256501af713287"
            ],
            "layout": "IPY_MODEL_67c896a60cc74b15a8f014ef40b4cd69"
          }
        },
        "d35fe5e2d12148eca09b38cde4ca71b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_107e4bd5150f4a75a02bc51590042c29",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6d8fea7a2eb84585ab540823f760663b",
            "value": "config.json:â€‡100%"
          }
        },
        "91d1531074574c5590fa946b7f3b600a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_057742f893a445ac9e31ed4600d78645",
            "max": 482,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cdb8c49edb8a49e8b062f9fd0014e5c2",
            "value": 482
          }
        },
        "5b41ca91021b45d2b3256501af713287": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc667b7cfab94afcace71b18842751da",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_797d01cd2aab4586a6b629b5895d76ee",
            "value": "â€‡482/482â€‡[00:00&lt;00:00,â€‡18.4kB/s]"
          }
        },
        "67c896a60cc74b15a8f014ef40b4cd69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "107e4bd5150f4a75a02bc51590042c29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d8fea7a2eb84585ab540823f760663b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "057742f893a445ac9e31ed4600d78645": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdb8c49edb8a49e8b062f9fd0014e5c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc667b7cfab94afcace71b18842751da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "797d01cd2aab4586a6b629b5895d76ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d8bc9c15ea449b7a0411e5486b9fa3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3135d45ec778461c972e520fc3f71479",
              "IPY_MODEL_89a05d73746e47759af05940076abc4d",
              "IPY_MODEL_152900b3214e4aeca8151118826251ae"
            ],
            "layout": "IPY_MODEL_b837e4f225df4bb8906e3c3b6535fb70"
          }
        },
        "3135d45ec778461c972e520fc3f71479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78b62b2e2dcd40ac99109c090f1fd316",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b20dbf4a1987425da4af87f3869510a2",
            "value": "vocab.json:â€‡100%"
          }
        },
        "89a05d73746e47759af05940076abc4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_392bef8628284a5aac1e093bab7c5b01",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28732882135149feb5d7f38100c5718e",
            "value": 898823
          }
        },
        "152900b3214e4aeca8151118826251ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdb59ae5a46d485ab32068dceba11db9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8bd8dede9c5b4ee7840f8f2006c15531",
            "value": "â€‡899k/899kâ€‡[00:00&lt;00:00,â€‡9.16MB/s]"
          }
        },
        "b837e4f225df4bb8906e3c3b6535fb70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78b62b2e2dcd40ac99109c090f1fd316": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b20dbf4a1987425da4af87f3869510a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "392bef8628284a5aac1e093bab7c5b01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28732882135149feb5d7f38100c5718e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bdb59ae5a46d485ab32068dceba11db9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bd8dede9c5b4ee7840f8f2006c15531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04ae56ad2db94b6f90aa74af9212f43d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44a126e9e7f84dc2a8273c68d04d34dc",
              "IPY_MODEL_d0575dd6f84449e797459cebdd4b7c4a",
              "IPY_MODEL_74505e3ed6ed4aa5bb98f2b552085e25"
            ],
            "layout": "IPY_MODEL_aa35d6eed25d448c8d70f89b3fa71e6a"
          }
        },
        "44a126e9e7f84dc2a8273c68d04d34dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c5b79e4edce4f6ebbac9285e5f54d82",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d219f44f176a4b0d84407d445889c513",
            "value": "merges.txt:â€‡100%"
          }
        },
        "d0575dd6f84449e797459cebdd4b7c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e2da9393bab431aa8a6ec88538df994",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aaf15bb4af0249ed992d9d9767dc718f",
            "value": 456318
          }
        },
        "74505e3ed6ed4aa5bb98f2b552085e25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb99140a3c2b456bb82966bacb3ddd19",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6df114b593fd425daaf17565040c74b8",
            "value": "â€‡456k/456kâ€‡[00:00&lt;00:00,â€‡8.69MB/s]"
          }
        },
        "aa35d6eed25d448c8d70f89b3fa71e6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c5b79e4edce4f6ebbac9285e5f54d82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d219f44f176a4b0d84407d445889c513": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e2da9393bab431aa8a6ec88538df994": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaf15bb4af0249ed992d9d9767dc718f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb99140a3c2b456bb82966bacb3ddd19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6df114b593fd425daaf17565040c74b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e10202b2eda4c1388d0e4380884211e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13bf210fd39649ba90541ccdf0218f9f",
              "IPY_MODEL_ae832281452d415aa35e3990ecf9354c",
              "IPY_MODEL_7db345d6f43144129f1b08ee38c389a5"
            ],
            "layout": "IPY_MODEL_d2ed695e503f4a5d9d29763e361d8f79"
          }
        },
        "13bf210fd39649ba90541ccdf0218f9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8c2f3aa3d1e4488af132dbe2d78e4ff",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_926988b57c7d4f8b9110b7c08f1fb21e",
            "value": "tokenizer.json:â€‡100%"
          }
        },
        "ae832281452d415aa35e3990ecf9354c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ec08fdb4cab4cc98417270a686a59e2",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f8d258000004bbea0f07ba5cf163d83",
            "value": 1355863
          }
        },
        "7db345d6f43144129f1b08ee38c389a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3938bb79f1de4090b25a00b9fd84e6ea",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0abe2f0df0d4477cbda5a5259d38dddb",
            "value": "â€‡1.36M/1.36Mâ€‡[00:00&lt;00:00,â€‡9.19MB/s]"
          }
        },
        "d2ed695e503f4a5d9d29763e361d8f79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8c2f3aa3d1e4488af132dbe2d78e4ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "926988b57c7d4f8b9110b7c08f1fb21e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ec08fdb4cab4cc98417270a686a59e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f8d258000004bbea0f07ba5cf163d83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3938bb79f1de4090b25a00b9fd84e6ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0abe2f0df0d4477cbda5a5259d38dddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "573830a41e594dda9f833e5c91c217a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c5ff2301ae884fafab2382adbb2af9e6",
              "IPY_MODEL_9c53857f43d34fa58d3acc77566b58c4",
              "IPY_MODEL_161faf0efcb34b6b8349d639eb0c2104"
            ],
            "layout": "IPY_MODEL_b3047e31a77a45a5903625b77d8a5088"
          }
        },
        "c5ff2301ae884fafab2382adbb2af9e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d89b03646b0b402594e5679d2de7da4f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e50583961eaf4995b1f443137ef575db",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "9c53857f43d34fa58d3acc77566b58c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fd5f8fd1ea04258bbf5b93cd4e5aed2",
            "max": 1421700479,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_659c9ffcbbc44ce486878c455f3b3e86",
            "value": 1421700479
          }
        },
        "161faf0efcb34b6b8349d639eb0c2104": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a64f20739306460e872b56f3ccbdd114",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_82cd2461d1e94ef19d15b4857c174014",
            "value": "â€‡1.42G/1.42Gâ€‡[00:44&lt;00:00,â€‡36.0MB/s]"
          }
        },
        "b3047e31a77a45a5903625b77d8a5088": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d89b03646b0b402594e5679d2de7da4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e50583961eaf4995b1f443137ef575db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fd5f8fd1ea04258bbf5b93cd4e5aed2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "659c9ffcbbc44ce486878c455f3b3e86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a64f20739306460e872b56f3ccbdd114": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82cd2461d1e94ef19d15b4857c174014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63a1f5ccebb1456ca9790ac52feb25e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d51b619c5d584f5d9d264f1ee1cf16a1",
              "IPY_MODEL_e8f33c3f955b4362a88e59229f82e852",
              "IPY_MODEL_1f41742143c8408a99e8f29d66ccef39"
            ],
            "layout": "IPY_MODEL_1291e449cf5649a1a05e9d1025f04c2b"
          }
        },
        "d51b619c5d584f5d9d264f1ee1cf16a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_331721b4bf184782b3141cc3c3a46223",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3d4117121d2f43f991c0085373741fbf",
            "value": "Batches:â€‡100%"
          }
        },
        "e8f33c3f955b4362a88e59229f82e852": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d980cc1d5eec4eb1994565409ee70013",
            "max": 813,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f032c63df2a24961b7530f554194f7af",
            "value": 813
          }
        },
        "1f41742143c8408a99e8f29d66ccef39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c3afbc1257048128682422d0dc68148",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8ca66032d3c541e58bfc385038fc9be8",
            "value": "â€‡813/813â€‡[00:09&lt;00:00,â€‡120.97it/s]"
          }
        },
        "1291e449cf5649a1a05e9d1025f04c2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "331721b4bf184782b3141cc3c3a46223": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d4117121d2f43f991c0085373741fbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d980cc1d5eec4eb1994565409ee70013": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f032c63df2a24961b7530f554194f7af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c3afbc1257048128682422d0dc68148": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ca66032d3c541e58bfc385038fc9be8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}